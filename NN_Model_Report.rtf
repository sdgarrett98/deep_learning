{\rtf1\ansi\ansicpg1252\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fmodern\fcharset0 Courier;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid2\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid102\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid103\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa298\partightenfactor0

\f0\b\fs36 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Report on the Deep Learning Model for Alphabet Soup\
\pard\pardeftab720\sa280\partightenfactor0

\fs28 \cf0 Overview of the Analysis\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 \strokec2 The purpose of this analysis was to develop a deep learning model that can predict whether applicants for funding from Alphabet Soup are likely to be successful. The goal is to leverage historical data to build a model capable of classifying applicants and aiding the decision-making process for fund allocation.\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 \strokec2 Results\
\pard\pardeftab720\sa319\partightenfactor0

\fs24 \cf0 Data Preprocessing\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls1\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Target Variable(s):
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The target variable for this model is likely a binary variable representing whether the application was successful (labeled as 
\f2\fs26 IS_SUCCESSFUL
\f1\fs24 ).\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls1\ilvl0
\f0\b \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Feature Variable(s):
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The feature variables include all other columns in the dataset that provide information about the applicants, such as their financial standing, organization type, and previous performance metrics. These features are used to predict the target variable.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls1\ilvl0
\f0\b \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Removed Variables:
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls1\ilvl1\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}Initially EIN and Name were dropped, and when optimizes SPECIAL_CONTRIBUTIONS_N, SPECIAL_CONTRIBUTIONS_Y, and STATUS were also dropped. \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\pard\tx720\tx1440\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\sa319\partightenfactor0

\f0\b \cf0 Compiling, Training, and Evaluating the Model\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Neurons, Layers, and Activation Functions:
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls2\ilvl1\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The neural network model contained:\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls2\ilvl2\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Several layers, including Dense layers with different numbers of neurons.\
\ls2\ilvl2\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The exact number of neurons varied across layers to capture complex patterns in the data. For example, the model might have used 80 neurons in the first hidden layer, 30 neurons in the second hidden layer, and so on.\
\ls2\ilvl2\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Activation functions such as relu and tanh were used for the hidden layers, while the output layer used a sigmoid activation function to produce a binary classification output.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls2\ilvl0
\f0\b \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Model Performance:
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls2\ilvl1\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The model achieved an accuracy of approximately 
\f0\b 64.87%
\f1\b0  on the test data, with a corresponding loss of 
\f0\b 0.6856
\f1\b0 . This indicates the model was able to perform above random chance (50%), but there is still significant room for improvement in terms of accuracy.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls2\ilvl0
\f0\b \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Steps Taken to Improve Performance:
\f1\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls2\ilvl1\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The following steps were likely taken to improve the model performance:\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls2\ilvl2
\f0\b \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Adjusting the network architecture:
\f1\b0  Changes to the number of neurons and layers to better capture the underlying data patterns.
\f0\b \
\ls2\ilvl2
\f1\b0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}
\f0\b \expnd0\expndtw0\kerning0
Activation Function tuning:
\f1\b0  The activation function was tweaked to evaluate reuy vs tanh performance, tanh lead to better accuracy overall. However, when using a combination of rely and tanh the order was a factor.\outl0\strokewidth0 \strokec2 \
\ls2\ilvl2
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Optimizer tuning:
\f1\b0  The optimization function (e.g., Adam or RMSprop) was tweaked to ensure the model converges better during training.\
\ls2\ilvl2
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Epoch count:
\f1\b0  Increasing or decreasing the number of training epochs to prevent overfitting or underfitting.\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 Summary\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 \strokec2 The deep learning model provided an accuracy of 
\f0\b \strokec2 64.87%
\f1\b0 \strokec2 , which, while better than random guessing, leaves significant room for improvement. The classification task might benefit from further hyperparameter tuning, feature engineering, or possibly using a different modeling approach altogether.\
}